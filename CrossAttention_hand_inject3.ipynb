{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b4974c-6437-422d-afae-daa2884ad633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个笔记本仅仅为了在多个gpu上跑不同的实验，加速用，笔记本内容思路基本一致\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# this notebook aim to inject attention map by hand\n",
    "#NOTE: Last tested working diffusers version is diffusers==0.4.1, https://github.com/huggingface/diffusers/releases/tag/v0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ebfbd7-5026-4830-93e5-d43272db8912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all models\n"
     ]
    }
   ],
   "source": [
    "#Init CLIP tokenizer and model\n",
    "model_path_clip = \"openai/clip-vit-large-patch14\"\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(model_path_clip)\n",
    "clip_model = CLIPModel.from_pretrained(model_path_clip, torch_dtype=torch.float16)\n",
    "clip = clip_model.text_model\n",
    "\n",
    "#Init diffusion model\n",
    "auth_token = \"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\" #Replace this with huggingface auth token as a string if model is not already downloaded\n",
    "model_path_diffusion = \"CompVis/stable-diffusion-v1-4\"\n",
    "unet = UNet2DConditionModel.from_pretrained(model_path_diffusion, subfolder=\"unet\", use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16)\n",
    "vae = AutoencoderKL.from_pretrained(model_path_diffusion, subfolder=\"vae\", use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "#Move to GPU\n",
    "device = \"cuda:1\"\n",
    "unet.to(device)\n",
    "vae.to(device)\n",
    "clip.to(device)\n",
    "print(\"Loaded all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a6f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "Debug = False\n",
    "\n",
    "\n",
    "# time maps include a serious of record attn maps\n",
    "time_maps = []\n",
    "record_attn_maps = []\n",
    "import os\n",
    "def mkdir(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "        os.makedirs(path)\n",
    "\n",
    "inject_map_animal = torch.tensor([\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "], dtype=torch.float16).to(device)\n",
    "\n",
    "\n",
    "\n",
    "inject_map_car = torch.tensor([\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "], dtype=torch.float16).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inject_map_animal_64 = inject_map_animal.repeat([8,1,1]).reshape([8,-1])\n",
    "inject_map_car_64 = inject_map_car.repeat([8,1,1]).reshape([8, -1])\n",
    "inject_map_animal_32 = inject_map_animal[::2,::2].repeat([8,1,1]).reshape([8,-1])\n",
    "inject_map_car_32 = inject_map_car[::2,::2].repeat([8,1,1]).reshape([8,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27cb0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject1(attention_scores, inject_map_dicts, inject_scale=3):\n",
    "    \"\"\"\n",
    "    absolute adding on softmaxed map, only on 64*64 maps\n",
    "    \"\"\"\n",
    "    attn_slice = attention_scores.softmax(dim=-1)\n",
    "    if attention_scores.shape[1] == 64*64:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map\"]\n",
    "            attn_slice[:,:,word_index] += inject_map * inject_scale\n",
    "    return attn_slice\n",
    "\n",
    "def inject2(attention_scores, inject_map_dicts, inject_scale=3):\n",
    "    \"\"\"\n",
    "    absolute replace on softmaxed map, only on 64*64 maps\n",
    "    \"\"\"\n",
    "    attn_slice = attention_scores.softmax(dim=-1)\n",
    "    if attention_scores.shape[1] == 64*64:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map\"]\n",
    "            attn_slice[:,:,word_index] = inject_map * inject_scale\n",
    "    return attn_slice\n",
    "\n",
    "def inject3(attention_scores, inject_map_dicts, inject_scale=30):\n",
    "    \"\"\"\n",
    "    absolute adding on attention scores, only on 64*64 maps\n",
    "    \"\"\"\n",
    "    if attention_scores.shape[1] == 64*64:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map\"]\n",
    "            attention_scores[:,:,word_index] += inject_map * inject_scale\n",
    "    \n",
    "    attn_slice = attention_scores.softmax(dim=-1)\n",
    "    return attn_slice\n",
    "\n",
    "def inject4(attention_scores, inject_map_dicts, inject_scale=30):\n",
    "    \"\"\"\n",
    "    absolute replacing on attention scores, only on 64*64 maps\n",
    "    \"\"\"\n",
    "    if attention_scores.shape[1] == 64*64:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map\"]\n",
    "            attention_scores[:,:,word_index] = inject_map * inject_scale\n",
    "    attn_slice = attention_scores.softmax(dim=-1)\n",
    "    return attn_slice\n",
    "\n",
    "\n",
    "\n",
    "def inject5(attention_scores, inject_map_dicts, inject_scale=3):\n",
    "    \"\"\"\n",
    "    absolute adding on softmaxed map, on both 32*32 and 64*64 maps\n",
    "    \"\"\"\n",
    "    attn_slice = attention_scores.softmax(dim=-1)\n",
    "    if attention_scores.shape[1] == 64*64:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map\"]\n",
    "            attn_slice[:,:,word_index] += inject_map * inject_scale\n",
    "    elif attention_scores.shape[1] == 32*32:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map_32\"]\n",
    "            attn_slice[:,:,word_index] += inject_map * inject_scale\n",
    "    return attn_slice\n",
    "\n",
    "def inject6(attention_scores, inject_map_dicts, inject_scale=3):\n",
    "    \"\"\"\n",
    "    absolute replace on softmaxed map, on both 32*32 and 64*64 maps\n",
    "    \"\"\"\n",
    "    attn_slice = attention_scores.softmax(dim=-1)\n",
    "    if attention_scores.shape[1] == 64*64:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map\"]\n",
    "            attn_slice[:,:,word_index] = inject_map * inject_scale\n",
    "    elif attention_scores.shape[1] == 32*32:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map_32\"]\n",
    "            attn_slice[:,:,word_index] = inject_map * inject_scale\n",
    "    return attn_slice\n",
    "\n",
    "def inject7(attention_scores, inject_map_dicts, inject_scale=30):\n",
    "    \"\"\"\n",
    "    absolute adding on attention scores, on both 32*32 and 64*64 maps\n",
    "    \"\"\"\n",
    "    if attention_scores.shape[1] == 64*64:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map\"]\n",
    "            attention_scores[:,:,word_index] += inject_map * inject_scale\n",
    "    elif attention_scores.shape[1] == 32*32:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map_32\"]\n",
    "            attention_scores[:,:,word_index] += inject_map * inject_scale\n",
    "    \n",
    "    attn_slice = attention_scores.softmax(dim=-1)\n",
    "    return attn_slice\n",
    "\n",
    "def inject8(attention_scores, inject_map_dicts, inject_scale=30):\n",
    "    \"\"\"\n",
    "    absolute replacing on attention scores, on both 32*32 and 64*64 maps\n",
    "    \"\"\"\n",
    "    if attention_scores.shape[1] == 64*64:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map\"]\n",
    "            attention_scores[:,:,word_index] = inject_map * inject_scale\n",
    "    elif attention_scores.shape[1] == 32*32:\n",
    "        for dic in inject_map_dicts:\n",
    "            word_index = dic[\"word_index\"]\n",
    "            inject_map = dic[\"inject_map_32\"]\n",
    "            attention_scores[:,:,word_index] = inject_map * inject_scale\n",
    "    attn_slice = attention_scores.softmax(dim=-1)\n",
    "    return attn_slice\n",
    "\n",
    "\n",
    "inject_functions = [inject1, inject2, inject3, inject4, inject5, inject5, inject6, inject7, inject8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08dee7d1-e050-43d3-86a9-5776276aad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def init_attention_weights(weight_tuples):\n",
    "    tokens_length = clip_tokenizer.model_max_length\n",
    "    weights = torch.ones(tokens_length)\n",
    "    \n",
    "    for i, w in weight_tuples:\n",
    "        if i < tokens_length and i >= 0:\n",
    "            weights[i] = w\n",
    "    \n",
    "    \n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.last_attn_slice_weights = weights.to(device)\n",
    "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
    "            module.last_attn_slice_weights = None\n",
    "    \n",
    "\n",
    "\n",
    "def save_map(attn_map):\n",
    "    record_attn_maps.append(attn_map)\n",
    "    \n",
    "attn_layers = 0\n",
    "attn2_layers = 0\n",
    "def init_attention_func():\n",
    "    attn_layers = 0\n",
    "    attn2_layers = 0\n",
    "    #ORIGINAL SOURCE CODE: https://github.com/huggingface/diffusers/blob/91ddd2a25b848df0fa1262d4f1cd98c7ccb87750/src/diffusers/models/attention.py#L276\n",
    "    def new_attention(self, query, key, value):\n",
    "        # TODO: use baddbmm for better performance\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) * self.scale\n",
    "        \n",
    "        if self.inject:\n",
    "            attn_slice = self.inject_func(attention_scores, self.inject_map_dicts, self.inject_scale)\n",
    "            self.inject = False\n",
    "        else:\n",
    "            attn_slice = attention_scores.softmax(dim=-1)\n",
    "\n",
    "        hidden_states = torch.matmul(attn_slice, value)\n",
    "        # reshape hidden_states\n",
    "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "    def new_sliced_attention(self, query, key, value, sequence_length, dim):\n",
    "        print(\"hello!\") # 这个函数貌似没有走进来过\n",
    "        batch_size_attention = query.shape[0]\n",
    "        hidden_states = torch.zeros(\n",
    "            (batch_size_attention, sequence_length, dim // self.heads), device=query.device, dtype=query.dtype\n",
    "        )\n",
    "        slice_size = self._slice_size if self._slice_size is not None else hidden_states.shape[0]\n",
    "        for i in range(hidden_states.shape[0] // slice_size):\n",
    "            start_idx = i * slice_size\n",
    "            end_idx = (i + 1) * slice_size\n",
    "            attn_slice = (\n",
    "                torch.matmul(query[start_idx:end_idx], key[start_idx:end_idx].transpose(1, 2)) * self.scale\n",
    "            )  # TODO: use baddbmm for better performance\n",
    "            attn_slice = attn_slice.softmax(dim=-1)\n",
    "            if self.use_last_attn_slice:\n",
    "                if self.last_attn_slice_mask is not None:\n",
    "                    new_attn_slice = torch.index_select(self.last_attn_slice, -1, self.last_attn_slice_indices)\n",
    "                    attn_slice = attn_slice * (1 - self.last_attn_slice_mask) + new_attn_slice * self.last_attn_slice_mask\n",
    "                else:\n",
    "                    attn_slice = self.last_attn_slice\n",
    "                self.use_last_attn_slice = False\n",
    "                    \n",
    "                \n",
    "            if self.use_last_attn_weights and self.last_attn_slice_weights is not None:\n",
    "                attn_slice = attn_slice * self.last_attn_slice_weights\n",
    "                self.use_last_attn_weights = False\n",
    "            \n",
    "            attn_slice = torch.matmul(attn_slice, value[start_idx:end_idx])\n",
    "\n",
    "            hidden_states[start_idx:end_idx] = attn_slice\n",
    "        # reshape hidden_states\n",
    "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\":\n",
    "            if \"attn2\" in name:\n",
    "                attn2_layers += 1\n",
    "                attn_type = \"attn2\"\n",
    "            else:\n",
    "                attn_type = \"attn1\"\n",
    "            attn_layers += 1\n",
    "            module.attn_order = attn_layers\n",
    "            module.attn_type = attn_type\n",
    "            module.inject = False\n",
    "            module.inject_scale = 0.0\n",
    "            module.inject_map_dicts = None\n",
    "            module.inject_func = None\n",
    "            module._sliced_attention = new_sliced_attention.__get__(module, type(module))\n",
    "            module._attention = new_attention.__get__(module, type(module))\n",
    "\n",
    "    \n",
    "def inject_map_by_hand(inject=True):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.inject = inject\n",
    "            \n",
    "def set_inject_map_dicts(map_dicts):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.inject_map_dicts = map_dicts\n",
    "\n",
    "\n",
    "def set_inject_func(func):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.inject_func = func\n",
    "            \n",
    "def set_inject_scale(scale):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.inject_scale = scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd6e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def stablediffusion(prompt, prompt_edit_token_weights=[], \n",
    "                    prompt_edit_tokens_start=0.0, prompt_edit_tokens_end=1.0, \n",
    "                    inject_map_dicts = [], inject_scale=0.0, inject_func=None,\n",
    "                    guidance_scale=7.5, steps=50, seed=None, width=512, height=512, \n",
    "                    init_image=None, init_image_strength=0.5):\n",
    "    #Change size to multiple of 64 to prevent size mismatches inside model\n",
    "    record_attn_maps.clear()\n",
    "    width = width - width % 64\n",
    "    height = height - height % 64\n",
    "    \n",
    "    #If seed is None, randomly select seed from 0 to 2^32-1\n",
    "    if seed is None: seed = random.randrange(2**32 - 1)\n",
    "    generator = torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    #Set inference timesteps to scheduler\n",
    "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(steps)\n",
    "    \n",
    "    #Preprocess image if it exists (img2img)\n",
    "    if init_image is not None:\n",
    "        #Resize and transpose for numpy b h w c -> torch b c h w\n",
    "        init_image = init_image.resize((width, height), resample=Image.Resampling.LANCZOS)\n",
    "        init_image = np.array(init_image).astype(np.float32) / 255.0 * 2.0 - 1.0\n",
    "        init_image = torch.from_numpy(init_image[np.newaxis, ...].transpose(0, 3, 1, 2))\n",
    "        \n",
    "        #If there is alpha channel, composite alpha for white, as the diffusion model does not support alpha channel\n",
    "        if init_image.shape[1] > 3:\n",
    "            init_image = init_image[:, :3] * init_image[:, 3:] + (1 - init_image[:, 3:])\n",
    "            \n",
    "        #Move image to GPU\n",
    "        init_image = init_image.to(device)\n",
    "        \n",
    "        #Encode image\n",
    "        with autocast(device):\n",
    "            init_latent = vae.encode(init_image).latent_dist.sample(generator=generator) * 0.18215\n",
    "            \n",
    "        t_start = steps - int(steps * init_image_strength)\n",
    "            \n",
    "    else:\n",
    "        init_latent = torch.zeros((1, unet.in_channels, height // 8, width // 8), device=device)\n",
    "        t_start = 0\n",
    "    \n",
    "    #Generate random normal noise\n",
    "    noise = torch.randn(init_latent.shape, generator=generator, device=device)\n",
    "    #latent = noise * scheduler.init_noise_sigma\n",
    "    latent = scheduler.add_noise(init_latent, noise, torch.tensor([scheduler.timesteps[t_start]], device=device)).to(device)\n",
    "    #Process clip\n",
    "    with autocast(\"cuda\"):\n",
    "        tokens_unconditional = clip_tokenizer(\"\", padding=\"max_length\", max_length=clip_tokenizer.model_max_length, truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True)\n",
    "        embedding_unconditional = clip(tokens_unconditional.input_ids.to(device)).last_hidden_state\n",
    "\n",
    "        tokens_conditional = clip_tokenizer(prompt, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True)\n",
    "        embedding_conditional = clip(tokens_conditional.input_ids.to(device)).last_hidden_state\n",
    "\n",
    "\n",
    "        \n",
    "        init_attention_func()\n",
    "        init_attention_weights(prompt_edit_token_weights)\n",
    "        set_inject_func(inject_func)\n",
    "        set_inject_scale(inject_scale)\n",
    "        set_inject_map_dicts(inject_map_dicts)\n",
    "        \n",
    "            \n",
    "        timesteps = scheduler.timesteps[t_start:]\n",
    "\n",
    "        \n",
    "        for i, t in tqdm(enumerate(timesteps), total=len(timesteps)):\n",
    "            t_index = t_start + i\n",
    "            t_scale = t / scheduler.num_train_timesteps\n",
    "            #sigma = scheduler.sigmas[t_index]\n",
    "            latent_model_input = latent\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "            \n",
    "            \n",
    "            #Predict the unconditional noise residual\n",
    "            noise_pred_uncond = unet(latent_model_input, t, encoder_hidden_states=embedding_unconditional).sample\n",
    "            \n",
    "            if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end:\n",
    "                inject_map_by_hand()\n",
    "            \n",
    "            noise_pred_cond = unet(latent_model_input, t, encoder_hidden_states=embedding_conditional).sample\n",
    "                \n",
    "            #Perform guidance\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "            \n",
    "\n",
    "            latent = scheduler.step(noise_pred, t_index, latent).prev_sample\n",
    "\n",
    "        #scale and decode the image latents with vae\n",
    "        latent = latent / 0.18215\n",
    "        image = vae.decode(latent.to(vae.dtype)).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "    image = (image[0] * 255).round().astype(\"uint8\")\n",
    "    \n",
    "    return Image.fromarray(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e9d39a-df21-45ba-bcfc-58b8784617d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_token(prompt, index):\n",
    "    tokens = clip_tokenizer(prompt, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True).input_ids[0]\n",
    "    print(len(tokens))\n",
    "    return clip_tokenizer.decode(tokens[index:index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44ff9cb-6d43-4b32-bf2a-17bfca665218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "car\n",
      "77\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "prompt_word_index = 6\n",
    "print(prompt_token(\"a cat sitting on a car\", prompt_word_index))\n",
    "print(prompt_token(\"a smiling dog sitting on a car\",prompt_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c377b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inject map dicts\n",
    "animal_dict = {\n",
    "    \"word_index\": 2,\n",
    "    \"inject_map\": inject_map_animal_64,\n",
    "    \"inject_map_32\": inject_map_animal_32\n",
    "}\n",
    "car_dict = {\n",
    "    \"word_index\":6,\n",
    "    \"inject_map\": inject_map_car_64,\n",
    "    \"inject_map_32\": inject_map_car_32\n",
    "}\n",
    "only_animal = [animal_dict]\n",
    "only_car = [car_dict]\n",
    "animal_car = [animal_dict, car_dict]\n",
    "\n",
    "map_dict_list = [only_animal, only_car, animal_car]\n",
    "name_list = [\"on-ani\",\"on-car\", \"an-car\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aba74234",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 24839640267\n",
    "mkdir(\"./hand_inject\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba17dce4-f14c-4df3-8325-7adcabb33c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8da0f1ffa1e499288def56fab23e318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef39865d0e41422b9f938af0a76fde3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21348041f74f475c8cf344a5367ab8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f75ee738b54f0ea9e5b5ef7ed727ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0958acc0c924407b853320efbcbabd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ac9b52a36f42afb86dbb4fe25bc211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7606b4ab1ad24960a41851ba2e89da64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48eb5c564fc45f285f52fa933525cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7096bc0026354fb38e86c0caa1ff031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea8400c78e3464cb3981057e9048dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cba1e008ee44740895c10da1f6e94eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b311107ad18f4af8a8550f7c056938da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f26aa168f024b94bc155a8320b0c437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5076184d6ea64842b5cf0a2e3a7f63bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c3cb615353423c87a90bb80d4a54a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5879d75a6a54b1aac43e9d125a01444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cfd48caea64b1c912383c780f11344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51a6d331d384103be0c9115f924290d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868f3bc725e24a659cf5fe9d3939232f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab9bab3576447d1b87129a53a64ea0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98387599ceb346bcbf796b3bc15ba828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20c521fb0104702b4466a2230800f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6884c579f5aa44a49b687984f67776dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f34433fe20540f4bd7cb11f88f58ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "func = inject5 # adding, both\n",
    "\n",
    "for i in range(len(map_dict_list)):\n",
    "    name = name_list[i]\n",
    "    inject_map_dicts = map_dict_list[i]\n",
    "    for scale in [1., 1.5,2., 2.3, 2.7, 3., 3.2, 3.5]:\n",
    "        res = stablediffusion(\"a hamster sitting on a car\",\n",
    "                inject_map_dicts = inject_map_dicts, inject_scale=scale, inject_func=func,\n",
    "                seed=seed, steps=50, prompt_edit_tokens_start=0.8)\n",
    "        mkdir(\"./hand_inject/maps-scale/\")\n",
    "        res.save(f\"./hand_inject/maps-scale/func5-cs0.8-ste50-{name}-scal{scale}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb871a4d-153f-4afe-b6fe-a4cdd4635021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
